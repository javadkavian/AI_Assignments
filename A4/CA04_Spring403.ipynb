{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI_Assignment #4\n",
        "## Machine Learning\n",
        "### Mohammad Javad Pesarakloo, SID = 810100103 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA\n",
        "At first, lets load the dataset as a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>MEDV</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
              "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296.0   \n",
              "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242.0   \n",
              "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242.0   \n",
              "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622    3  222.0   \n",
              "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622    3  222.0   \n",
              "\n",
              "   PTRATIO       B  LSTAT  MEDV  \n",
              "0     15.3  396.90   4.98  24.0  \n",
              "1     17.8  396.90   9.14  21.6  \n",
              "2     17.8  392.83   4.03  34.7  \n",
              "3     18.7     NaN   2.94  33.4  \n",
              "4     18.7  396.90   5.33  36.2  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_excel('DataSet.xlsx')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 506 entries, 0 to 505\n",
            "Data columns (total 14 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   CRIM     506 non-null    float64\n",
            " 1   ZN       506 non-null    float64\n",
            " 2   INDUS    506 non-null    float64\n",
            " 3   CHAS     480 non-null    float64\n",
            " 4   NOX      506 non-null    float64\n",
            " 5   RM       506 non-null    float64\n",
            " 6   AGE      506 non-null    float64\n",
            " 7   DIS      479 non-null    float64\n",
            " 8   RAD      506 non-null    int64  \n",
            " 9   TAX      506 non-null    float64\n",
            " 10  PTRATIO  506 non-null    float64\n",
            " 11  B        486 non-null    float64\n",
            " 12  LSTAT    506 non-null    float64\n",
            " 13  MEDV     452 non-null    float64\n",
            "dtypes: float64(13), int64(1)\n",
            "memory usage: 55.5 KB\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>MEDV</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>480.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>479.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>486.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>452.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.269195</td>\n",
              "      <td>13.295257</td>\n",
              "      <td>9.205158</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>1.101175</td>\n",
              "      <td>15.679800</td>\n",
              "      <td>58.744660</td>\n",
              "      <td>6.211663</td>\n",
              "      <td>78.063241</td>\n",
              "      <td>339.317787</td>\n",
              "      <td>42.614980</td>\n",
              "      <td>336.820947</td>\n",
              "      <td>11.537806</td>\n",
              "      <td>23.750442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.399207</td>\n",
              "      <td>23.048697</td>\n",
              "      <td>7.169630</td>\n",
              "      <td>0.380364</td>\n",
              "      <td>1.646991</td>\n",
              "      <td>27.220206</td>\n",
              "      <td>33.104049</td>\n",
              "      <td>6.527286</td>\n",
              "      <td>203.542157</td>\n",
              "      <td>180.670077</td>\n",
              "      <td>87.585243</td>\n",
              "      <td>121.174519</td>\n",
              "      <td>6.064932</td>\n",
              "      <td>8.808602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.385000</td>\n",
              "      <td>3.561000</td>\n",
              "      <td>1.137000</td>\n",
              "      <td>1.129600</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.200000</td>\n",
              "      <td>2.600000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>1.730000</td>\n",
              "      <td>6.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.049443</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.440000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.449000</td>\n",
              "      <td>5.961500</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>2.425900</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>370.415000</td>\n",
              "      <td>6.877500</td>\n",
              "      <td>18.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.144655</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.960000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>6.322500</td>\n",
              "      <td>65.250000</td>\n",
              "      <td>3.917500</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>307.000000</td>\n",
              "      <td>18.900000</td>\n",
              "      <td>390.885000</td>\n",
              "      <td>10.380000</td>\n",
              "      <td>21.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.819623</td>\n",
              "      <td>18.100000</td>\n",
              "      <td>18.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.647000</td>\n",
              "      <td>6.949000</td>\n",
              "      <td>89.975000</td>\n",
              "      <td>6.341400</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>403.000000</td>\n",
              "      <td>20.200000</td>\n",
              "      <td>395.630000</td>\n",
              "      <td>15.015000</td>\n",
              "      <td>26.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.966540</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>27.740000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.313000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>666.000000</td>\n",
              "      <td>711.000000</td>\n",
              "      <td>396.900000</td>\n",
              "      <td>396.900000</td>\n",
              "      <td>34.410000</td>\n",
              "      <td>50.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
              "count  506.000000  506.000000  506.000000  480.000000  506.000000  506.000000   \n",
              "mean     1.269195   13.295257    9.205158    0.175000    1.101175   15.679800   \n",
              "std      2.399207   23.048697    7.169630    0.380364    1.646991   27.220206   \n",
              "min      0.000000    0.000000    0.000000    0.000000    0.385000    3.561000   \n",
              "25%      0.049443    0.000000    3.440000    0.000000    0.449000    5.961500   \n",
              "50%      0.144655    0.000000    6.960000    0.000000    0.538000    6.322500   \n",
              "75%      0.819623   18.100000   18.100000    0.000000    0.647000    6.949000   \n",
              "max      9.966540  100.000000   27.740000    1.000000    7.313000  100.000000   \n",
              "\n",
              "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
              "count  506.000000  479.000000  506.000000  506.000000  506.000000  486.000000   \n",
              "mean    58.744660    6.211663   78.063241  339.317787   42.614980  336.820947   \n",
              "std     33.104049    6.527286  203.542157  180.670077   87.585243  121.174519   \n",
              "min      1.137000    1.129600    1.000000   20.200000    2.600000    0.320000   \n",
              "25%     32.000000    2.425900    4.000000  254.000000   17.000000  370.415000   \n",
              "50%     65.250000    3.917500    5.000000  307.000000   18.900000  390.885000   \n",
              "75%     89.975000    6.341400   24.000000  403.000000   20.200000  395.630000   \n",
              "max    100.000000   24.000000  666.000000  711.000000  396.900000  396.900000   \n",
              "\n",
              "            LSTAT        MEDV  \n",
              "count  506.000000  452.000000  \n",
              "mean    11.537806   23.750442  \n",
              "std      6.064932    8.808602  \n",
              "min      1.730000    6.300000  \n",
              "25%      6.877500   18.500000  \n",
              "50%     10.380000   21.950000  \n",
              "75%     15.015000   26.600000  \n",
              "max     34.410000   50.000000  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.info()\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have an overall view of the data, lets find missing datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CRIM        0\n",
              "ZN          0\n",
              "INDUS       0\n",
              "CHAS       26\n",
              "NOX         0\n",
              "RM          0\n",
              "AGE         0\n",
              "DIS        27\n",
              "RAD         0\n",
              "TAX         0\n",
              "PTRATIO     0\n",
              "B          20\n",
              "LSTAT       0\n",
              "MEDV       54\n",
              "dtype: int64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we see, we have several missing items in our dataset.Now lets visualize unique values of each coloumn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CRIM  :  452\n",
            "ZN  :  27\n",
            "INDUS  :  77\n",
            "CHAS  :  3\n",
            "NOX  :  132\n",
            "RM  :  437\n",
            "AGE  :  399\n",
            "DIS  :  344\n",
            "RAD  :  10\n",
            "TAX  :  67\n",
            "PTRATIO  :  85\n",
            "B  :  358\n",
            "LSTAT  :  445\n",
            "MEDV  :  211\n"
          ]
        }
      ],
      "source": [
        "values = {x:np.unique(df[x].to_numpy()) for x in df.columns.tolist()}\n",
        "for key in values.keys():\n",
        "    print(key, \" : \", len(values[key]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6o5N2vfjkAN"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOZ_t3DjjnQz"
      },
      "source": [
        "Main form of simple linear regression function:\n",
        "$$f(x) = \\alpha x + \\beta$$\n",
        "\n",
        "here we want to find the bias ($\\alpha$) and slope($\\beta$) by minimizing the derivation of the Residual Sum of Squares (RSS) function:\n",
        "\n",
        "- step 1: Compute RSS of the training data  \n",
        "\n",
        "$$ RSS = \\Sigma (y_i - (\\hat{\\beta} + \\hat{\\alpha} * x_i) )^2 $$\n",
        "\n",
        "- step 2: Compute the derivatives of the RSS function in terms of $\\alpha$ and $\\beta$, and set them equal to 0 to find the desired parameters\n",
        "\n",
        "$$ \\frac{\\partial RSS}{\\partial \\beta} = \\Sigma (-f(x_i) + \\hat{\\beta} + \\hat{\\alpha} * x_i) = 0$$\n",
        "$$ \\to \\beta = \\hat{y} - \\hat{\\alpha} \\hat{x} \\to (1)$$\n",
        "\n",
        "\n",
        "$$ \\frac{\\partial RSS}{\\partial \\alpha} = \\Sigma (-2 x_i y_i + 2 \\hat{\\beta} x_i + 2\\hat{\\alpha} x_i ^ 2) = 0 \\to (2)$$\n",
        "\n",
        "$$ (1) , (2) \\to \\hat{\\alpha} = \\frac{\\Sigma{(x_i - \\hat{x})(y_i - \\hat{y})}}{\\Sigma{(x_i - \\hat{x})^2}}\n",
        "$$\n",
        "$$ \\hat{\\beta} = y - \\hat{a} x$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PosEWpa7j1Z8"
      },
      "source": [
        "Based on the above formula, implement the function below to compute the parameters of a simple linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tgq8tHPXjkkk"
      },
      "outputs": [],
      "source": [
        "def linear_regression(input, output):\n",
        "  alpha = (np.sum((input - np.mean(input))*(output - np.mean(output)))) / (np.sum(np.power(input - np.mean(input), 2)))\n",
        "  beta = np.mean(output) - alpha*np.mean(input)\n",
        "  return [alpha, beta]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL5v360zklZ7"
      },
      "source": [
        "Now complete this `get_regression_predictions(...)` function to predict the value of given data based on the calculated intercept and slope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2cq1rghRkhKs"
      },
      "outputs": [],
      "source": [
        "def get_regression_predictions(input, intercept, slope):\n",
        "    return input*slope + intercept"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwXzz8dDkv7c"
      },
      "source": [
        "Now that we have a model and can make predictions, let's evaluate our model using Root Mean Square Error (RMSE). RMSE is the square root of the mean of the squared differences between the residuals, and the residuals is just a fancy word for the difference between the predicted output and the true output.\n",
        "\n",
        "Complete the following function to compute the RSME of a simple linear regression model given the input_feature, output, intercept and slope:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nujfA_y3kves"
      },
      "outputs": [],
      "source": [
        "def get_root_mean_square_error(predicted_values, actual_values):\n",
        "  return np.sqrt((1/(len(actual_values))) * np.sum(np.power(predicted_values - actual_values, 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByK1k1X1k1YE"
      },
      "source": [
        "The RMSE has no bound, thus it becomes challenging to determine whether a particular RMSE value is considered good or bad without any reference point. Instead, we use R2 score. The R2 score is calculated by comparing the sum of the squared differences between the actual and predicted values of the dependent variable to the total sum of squared differences between the actual and mean values of the dependent variable. The R2 score is formulated as below:\n",
        "\n",
        "$$R^2 = 1 - \\frac{SSres}{SStot} = 1 - \\frac{\\sum_{i=1}^{n} (y_{i,true} - y_{i,pred})^2}{\\sum_{i=1}^{n} (y_{i,true} - \\bar{y}_{true})^2} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B70NqPIkk7P0"
      },
      "source": [
        "Complete the following function to calculate the R2 score of a given input_feature, output, bias, and slope:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rRFf4yqmk9Jz"
      },
      "outputs": [],
      "source": [
        "def get_r2_score(predicted_values, actual_values):\n",
        "  mean_yMain = sum(actual_values) / len(actual_values)\n",
        "    \n",
        "  tss = sum((y - mean_yMain) ** 2 for y in actual_values)\n",
        "    \n",
        "  rss = sum((actual_values[i] - predicted_values[i]) ** 2 for i in range(len(actual_values)))\n",
        "  \n",
        "  r2 = 1 - (rss / tss)\n",
        "  \n",
        "  return r2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxOOUjmvk_bT"
      },
      "source": [
        "Now calculate the fitness of the model.\n",
        "Remember to provide explanation for the outputs in your code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S38EwioclBiV"
      },
      "outputs": [],
      "source": [
        "designated_feature_list = [] # ToDo\n",
        "\n",
        "for feature in designated_feature_list:\n",
        "  pass\n",
        "  #TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5xJaLcqkYNk"
      },
      "source": [
        "# Ploynomial Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkVMCH41kLdz"
      },
      "source": [
        "To extend the simple linear regression to polynomial regression, we can model the relationship between the independent variable $x$ and the dependent variable $y$ as a polynomial function of degree $n$:\n",
        "\n",
        "$$f(x) = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\ldots + \\beta_nx^n$$\n",
        "\n",
        "The steps to find the parameters $\\beta_i$ are similar to those in simple linear regression. We again minimize the RSS function by taking the derivatives with respect to each parameter and setting them to 0.\n",
        "\n",
        "- Step 1: Compute the RSS function for polynomial regression:\n",
        "\n",
        "$$ RSS = \\Sigma (y_i - (\\hat{\\beta_0} + \\hat{\\beta_1}x_i + \\hat{\\beta_2}x_i^2 + \\ldots + \\hat{\\beta_n}x_i^n))^2 $$\n",
        "\n",
        "- Step 2: Compute the derivatives of the RSS function with respect to each parameter $\\beta_i$ and set them to 0 to find the desired parameters.\n",
        "\n",
        "$$ \\frac{\\partial RSS}{\\partial \\beta_i} = 0, \\text{ for } i = 0, 1, 2, \\ldots, n$$\n",
        "\n",
        "Solving these equations will give us the optimal values of $\\beta_i$ for the polynomial regression model. The specific form of the equations will depend on the degree of the polynomial and the number of parameters.\n",
        "\n",
        "The general form for finding the coefficients for polynomial regression can be represented as:\n",
        "\n",
        "$$ \\beta = (X^T X)^{-1} X^T y $$\n",
        "\n",
        "where:\n",
        "- $X$ is the design matrix with columns $x^0, x^1, x^2, ..., x^n$\n",
        "- $x^i$ represents the feature vector of $x$ raised to the power of $i$\n",
        "- $y$ is the target variable vector\n",
        "- $\\beta$ is the coefficient vector for the polynomial regression\n",
        "\n",
        "By solving for $\\beta$ using the above formula, we can obtain the coefficients for the polynomial regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z4LNTn2tkMHO"
      },
      "outputs": [],
      "source": [
        "def polynomial_regression(x, y, degree):\n",
        "  X = np.array([np.power(x, i) for i in range(degree+1)])\n",
        "  return np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqETl66ong-U"
      },
      "source": [
        "## Computing the Derivative\n",
        "\n",
        "As we saw, the cost function is the sum over the data points of the squared difference between an observed output and a predicted output.\n",
        "\n",
        "Since the derivative of a sum is the sum of the derivatives, we can compute the derivative for a single data point and then sum over data points. We can write the squared difference between the observed output and predicted output for a single point as follows:\n",
        "\n",
        "$$\n",
        "(output  - (const* w _{0} + [feature_1] * w_{1} + ...+ [feature_n] * w_{n}  ))^2\n",
        "$$\n",
        "\n",
        "With n feautures and a const , So the derivative will be :\n",
        "\n",
        "\n",
        "$$\n",
        "2 * (output  - (const* w _{0} + [feature_1] * w_{1} + ...+ [feature_n] * w_{n}  ))\n",
        "$$\n",
        "\n",
        "The term inside the paranethesis is just the error (difference between prediction and output). So we can re-write this as:\n",
        "\n",
        "$$2 * error*[feature_i] $$\n",
        "\n",
        "\n",
        "That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself. In the case of the constant then this is just twice the sum of the errors!\n",
        "\n",
        "Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors.\n",
        "\n",
        "\n",
        "With this in mind, complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcfkt5wZnrIf"
      },
      "outputs": [],
      "source": [
        "def feature_derivative(errors, feature):\n",
        "  #TO DO\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pyRt-sBlVDt"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Now we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of increase and therefore the negative gradient is the direction of decrease and we're trying to minimize a cost function.\n",
        "\n",
        "\n",
        "The amount by which we move in the negative gradient direction is called the 'step size'. We stop when we are 'sufficiently close' to the optimum. We define this by requiring that the magnitude (length) of the gradient vector to be smaller than a fixed 'tolerance'.\n",
        "\n",
        "\n",
        "With this in mind, complete the following gradient descent function below using your derivative function above. For each step in the gradient descent we update the weight for each feature befofe computing our stopping criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4das-ABlaaP"
      },
      "outputs": [],
      "source": [
        "# Utility functions for multiple regression\n",
        "\n",
        "def normalize_features(chosen_features, data_frame):\n",
        "    for feature in chosen_features:\n",
        "        data_frame[feature] = (data_frame[feature] - data_frame[feature].mean()) / data_frame[feature].std()\n",
        "    return data_frame\n",
        "\n",
        "def predict_output(feature_matrix, weights, bias):\n",
        "    #TO DO FOR POLYNOMIAL REGRESSION PREDICTION\n",
        "    return predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "divz9_fEmjma"
      },
      "source": [
        "## Polynomial Regression Using Gradient Descent\n",
        "\n",
        "Polynomial regression using gradient descent involves finding the optimal parameters for a polynomial model by iteratively updating them based on the gradient of a loss function, typically the Mean Squared Error (MSE). The steps involved are as follows:\n",
        "\n",
        "- **Step 1: Define the polynomial model**\n",
        "The polynomial model has the form:\n",
        "$$f(x) = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\ldots + \\beta_nx^n$$\n",
        "\n",
        "- **Step 2: Define the loss function**\n",
        "The loss function, such as Mean Squared Error (MSE), measures the error between the actual target values and the predicted values by the model.\n",
        "\n",
        "- **Step 3: Initialize the coefficients**\n",
        "Start with initial guesses for the coefficients $\\beta_0, \\beta_1, \\ldots, \\beta_n$\n",
        "\n",
        "- **Step 4: Update the coefficients using Gradient Descent**\n",
        "Iteratively update the coefficients to minimize the loss function. This is done by computing the gradient of the loss function with respect to each coefficient and making small adjustments in the opposite direction of the gradient.\n",
        "\n",
        "- **Step 5: Repeat until convergence**\n",
        "Continue updating the coefficients iteratively until the algorithm converges to the optimal values.\n",
        "\n",
        "- **Step 6: Use the learned coefficients for prediction**\n",
        "Once the coefficients converge, they can be used in the polynomial function to make predictions on new data points.\n",
        "\n",
        "Overall, polynomial regression using gradient descent is an iterative optimization process that aims to find the best-fitting polynomial curve to the data points by minimizing the prediction errors. The learning rate and the number of iterations are key hyperparameters to tune for efficient convergence and accurate modeling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EdzTgrzlWlR"
      },
      "outputs": [],
      "source": [
        "def polynomial_regression_gradient_descent(feature_matrix, outputs, initial_weights,bias, step_size, tolerance):\n",
        "    weights = np.array(initial_weights)\n",
        "\n",
        "    while True:\n",
        "        # Compute predictions using polynomial function and errors\n",
        "        #TO DO\n",
        "\n",
        "        # Compute derivatives for all weights\n",
        "        #TO DO\n",
        "\n",
        "        # Update weights and bias\n",
        "        #TO DO\n",
        "\n",
        "        # Check convergence\n",
        "        #TO DO\n",
        "\n",
        "    return weights, bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QAqzvnEltkW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_polynomial_regression(chosen_feature_matrix, target_matrix, keywords):\n",
        "    initial_weights = keywords['initial_weights']\n",
        "    step_size = keywords['step_size']\n",
        "    tolerance = keywords['tolerance']\n",
        "    bias = keywords['bias']\n",
        "    weights = np.array(initial_weights)\n",
        "    weights, bias = polynomial_regression_gradient_descent(chosen_feature_matrix, target_matrix, weights, bias, step_size, tolerance)\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def get_weights_and_bias(chosen_features):\n",
        "\n",
        "    keywords = {\n",
        "        'initial_weights': np.array([.5]*len(chosen_features)),\n",
        "        'step_size': 1.e-4,\n",
        "        'tolerance': 1.e-10,\n",
        "        'bias': 0\n",
        "    }\n",
        "\n",
        "    # TO DO\n",
        "\n",
        "    return chosen_feature_matrix, train_weights, bias"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
